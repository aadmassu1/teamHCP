{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installing necessary dependencies\n",
    "import sys\n",
    "# !conda install --yes --prefix {sys.prefix} PIL\n",
    "# # !{sys.executable} -m pip install ____\n",
    "\n",
    "\n",
    "# !{sys.executable} -m pip install pytesseract\n",
    "# !{sys.executable} -m pip install pillow\n",
    "# !{sys.executable} -m pip install python-poppler\n",
    "# !{sys.executable} -m pip install opencv-python\n",
    "# !{sys.executable} -m pip install --upgrade pip\n",
    "# !conda install -c conda-forge poppler\n",
    "\n",
    "# !{sys.executable} -m pip install scipy\n",
    "# !{sys.executable} -m pip install sklearn\n",
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install nltk\n",
    "# !{sys.executable} -m pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyPDF2\n",
    "# import re\n",
    "# import win32com.client\n",
    "# from win32com.client.dynamic import Dispatch\n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "# import pandas as pd\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "# import numpy as np\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FUnction to extract words from a pdf and save it in as a text file\n",
    "\n",
    "def pdf_2_txt(path):          \n",
    "    for file in os.listdir(path): # goes through the folders'\n",
    "        if file.endswith('.pdf'): # finds pdfs\n",
    "            p = os.path.join(path,file) # grabs path of pdf\n",
    "            pdfpages = convert_from_path(p, 500) # Converting each page in the pdf into image file with 500 dpi\n",
    "            image_counter = 1\n",
    "            for page in pdfpages:\n",
    "                filename = f\"page_{image_counter}.jpg\"\n",
    "                # Save the image of the page in system\n",
    "                page.save(filename, 'JPEG')\n",
    "                image_counter+=1\n",
    "                text = str(((pytesseract.image_to_string(Image.open(filename))))) # takes image and process into text\n",
    "                # delete image\n",
    "                os.remove(filename)\n",
    "                text = text.replace('-\\n', '') # some text processing\n",
    "                                \n",
    "                # Checking if the page contains the \"Course objective or \n",
    "                # description or outline or goals or Learning Outcomes followed by an \"end of string\" and\n",
    "                # a new line (As a title)\"\n",
    "                \n",
    "                if re.match(re.compile(\"^Course\\s(Outline|Objectives|Objective|Description|Goals)$\\n\"),\n",
    "                            text) or re.match(re.compile(\"^Learing Outcomes$\\n\"), text):\n",
    "                    filename = filename.replace('.jpg','') # Naming\n",
    "                    with open(f'{filename}.txt', 'w+') as f: # Opens a text file and writes the characters onto\n",
    "                        print(text, file=f)\n",
    "                        print(f'Page_{image_counter}.txt has been saved into {os.getcwd()}')\n",
    "                    break # stops iteration as soon as page is found               \n",
    "                \n",
    "    return #text\n",
    "                    \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page_3.txt has been saved into C:\\Users\\15713\\Desktop\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# path = os.path.join(os.getcwd(),'syllabus.pdf')\n",
    "path = os.getcwd()\n",
    "pdf_2_txt(path)\n",
    "# pages = convert_from_path(path, 500) \n",
    "# for page in pages:\n",
    "#     page.save('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, I try to do some text processing and tokenization to see word frequencies\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import copy\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from __future__ import division\n",
    "import matplotlib\n",
    "\n",
    "# % matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\15713\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # downloads all stopwords to avoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pages(path):\n",
    "    # Combining pages\n",
    "    pages = ''\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.txt'):\n",
    "            with codecs.open(file, 'r') as f: \n",
    "                page = f.read()\n",
    "                pages+=page\n",
    "    return pages\n",
    "\n",
    "# pages = combine_pages(path)\n",
    "# pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = pages\n",
    "# tokens_P = WordPunctTokenizer().tokenize(PorterStemmer().stem(text))\n",
    "# tokens = WordPunctTokenizer().tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a list of unecessary words like 'the', 'or',... to avoid\n",
    "avoid = stopwords.words('english')\n",
    "avoid.append('edu') # adding our own word to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a text file into sentences or phrases\n",
    "def get_sentence_counter(path):\n",
    "    path = os.getcwd()\n",
    "    for file in os.listdir():\n",
    "        if file.endswith('.txt'):\n",
    "            with codecs.open(file, 'r') as f: \n",
    "                text = f.read()\n",
    "                # groups all words and white-spaces, then finds words until a punctuation is reached\n",
    "                regex_of_sentence = re.findall('([\\w\\s]{0,})[^\\w\\s]', text) \n",
    "                avoid = re.findall('\\d', text)\n",
    "                avoid.append('')\n",
    "    regex_of_sentence = [x for x in regex_of_sentence if x not in avoid]\n",
    "    # for i in regex_of_sentence:\n",
    "    #     print(i)\n",
    "    return collections.Counter(regex_of_sentence), len(regex_of_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a text file into words\n",
    "def get_text_counter(text, avoid, word_pattern):\n",
    "#     tokens = WordPunctTokenizer().tokenize(PorterStemmer().stem(text))\n",
    "    tokens = WordPunctTokenizer().tokenize(text)\n",
    "    tokens = list(map(lambda x: x.lower(), tokens)) # converting everything to lower-case\n",
    "    # cleaning up tokens by getting rid of stopwords and finding word characters\n",
    "    Puncs = [token for token in tokens if re.match(re.compile(\"\\W\"), token)] # finding all punctuations\n",
    "    Nums = [token for token in tokens if re.match(re.compile(\"\\d\"), token)] # finding all digit characters\n",
    "    # adding punctuations and digits to the list of avoided characters/words\n",
    "    avoid = Puncs + Nums\n",
    "    tokens = [token for token in tokens if re.match(word_pattern, token) and token not in avoid] \n",
    "    # collections.counter() returns the tokens and their frequency as a dictionary ({freq: token})\n",
    "    return collections.Counter(tokens), len(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make dataframe for both tokenized words and sentences with their abs and relative freqz\n",
    "def make_df(counter, size, ind_name):\n",
    "    abs_freq = np.array([x[1] for x in counter]) # the absolute freqz is found from the collections.counter()\n",
    "    rel_freq = abs_freq/size # relative freqz is calculated by dividing abs_freqz by the size\n",
    "    index = [x[0] for x in counter] # the tokens are used as an index \n",
    "    # creating the dataframe\n",
    "    df = pd.DataFrame(data=np.array([abs_freq, rel_freq]).T, \n",
    "                      index=index,\n",
    "                      columns=[\"Absolute frequency\",\"Relative frequency\"])\n",
    "    # Naming the index title\n",
    "    df.index.name = ind_name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Absolute frequency</th>\n",
       "      <th>Relative frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Least Common Words</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Your grade will be determined by your participation in discussions and submitted\\nreviews</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>You will be a reviewer or reader for 4 grants and participate in a simulated NIH\\nstudy section</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\nAssignment</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NSF and NIH pdfs and web sites</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Writing and Reviewing Research Proposals\\nReading</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\n6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quiz on Leedy Ch 11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leedy Ch11 and completion of the Linked In course on R\\n\\nEvaluation</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantitative Analyses and Review of Statistical Procedures\\nReading</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7\\n\\n5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quiz on Leedy Ch 4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perk\\nEvaluation</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>editors</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>com</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elsevier</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Publication Ethics from Elsevier web site\\n\\nhttps</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sections of Leedy Ch 4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\nReading</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research Ethics</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Confounding Variables</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sources of Bias</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\nSampling</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experimental Design</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantitative and Qualitative Research</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Research Process</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\n4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>List and discuss Strengths and Weaknesses of the Writing</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s suggestions</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>according to the Baylor Sections and identify where the paper corresponds and where it\\ndoes not to Baylor</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from your Literature\\nReview</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class presentation of an analysis of one paper</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 Bates and 3 Borja pdfs</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Writing a Manuscript for Publication\\nReading</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\n3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MS\\ntopic due in 4 weeks</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on your PhD</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25 references</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 pages</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Literature review report</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skim Leedy Ch3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\nReading</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by Chris Magee</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\nTraining on Reference Manager Software Zotero</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by Theresa Calcagno</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How to Do a Literature Review\\n\\nTraining on Library databases for literature searches</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2\\n\\n2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quiz on Leedy Chapters</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in class</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdfs</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Absolute frequency  \\\n",
       "Least Common Words                                                       \n",
       " Your grade will be determined by your particip...                 1.0   \n",
       " You will be a reviewer or reader for 4 grants ...                 1.0   \n",
       "\\n\\nAssignment                                                     1.0   \n",
       " NSF and NIH pdfs and web sites                                    1.0   \n",
       " Writing and Reviewing Research Proposals\\nRead...                 1.0   \n",
       "\\n\\n6                                                              1.0   \n",
       " quiz on Leedy Ch 11                                               1.0   \n",
       " Leedy Ch11 and completion of the Linked In cou...                 1.0   \n",
       " Quantitative Analyses and Review of Statistica...                 1.0   \n",
       "7\\n\\n5                                                             1.0   \n",
       " quiz on Leedy Ch 4                                                1.0   \n",
       "perk\\nEvaluation                                                   1.0   \n",
       "editors                                                            1.0   \n",
       "com                                                                1.0   \n",
       "elsevier                                                           1.0   \n",
       "www                                                                1.0   \n",
       " Publication Ethics from Elsevier web site\\n\\nh...                 1.0   \n",
       " Sections of Leedy Ch 4                                            1.0   \n",
       "\\nReading                                                          1.0   \n",
       " Research Ethics                                                   1.0   \n",
       " Confounding Variables                                             1.0   \n",
       " Sources of Bias                                                   1.0   \n",
       "\\nSampling                                                         1.0   \n",
       " Experimental Design                                               1.0   \n",
       " Quantitative and Qualitative Research                             1.0   \n",
       " The Research Process                                              1.0   \n",
       "\\n\\n4                                                              1.0   \n",
       " List and discuss Strengths and Weaknesses of t...                 1.0   \n",
       "s suggestions                                                      1.0   \n",
       " according to the Baylor Sections and identify ...                 1.0   \n",
       "from your Literature\\nReview                                       1.0   \n",
       " Class presentation of an analysis of one paper                    1.0   \n",
       " 4 Bates and 3 Borja pdfs                                          1.0   \n",
       " Writing a Manuscript for Publication\\nReading                     1.0   \n",
       "\\n\\n3                                                              1.0   \n",
       "MS\\ntopic due in 4 weeks                                           1.0   \n",
       " on your PhD                                                       1.0   \n",
       " 25 references                                                     1.0   \n",
       "5 pages                                                            1.0   \n",
       " Literature review report                                          1.0   \n",
       " Skim Leedy Ch3                                                    1.0   \n",
       "\\n\\nReading                                                        1.0   \n",
       "by Chris Magee                                                     1.0   \n",
       "\\n\\nTraining on Reference Manager Software Zotero                  1.0   \n",
       "by Theresa Calcagno                                                1.0   \n",
       " How to Do a Literature Review\\n\\nTraining on L...                 1.0   \n",
       " 2\\n\\n2                                                            1.0   \n",
       " quiz on Leedy Chapters                                            1.0   \n",
       " in class                                                          1.0   \n",
       "pdfs                                                               1.0   \n",
       "\n",
       "                                                    Relative frequency  \n",
       "Least Common Words                                                      \n",
       " Your grade will be determined by your particip...            0.011494  \n",
       " You will be a reviewer or reader for 4 grants ...            0.011494  \n",
       "\\n\\nAssignment                                                0.011494  \n",
       " NSF and NIH pdfs and web sites                               0.011494  \n",
       " Writing and Reviewing Research Proposals\\nRead...            0.011494  \n",
       "\\n\\n6                                                         0.011494  \n",
       " quiz on Leedy Ch 11                                          0.011494  \n",
       " Leedy Ch11 and completion of the Linked In cou...            0.011494  \n",
       " Quantitative Analyses and Review of Statistica...            0.011494  \n",
       "7\\n\\n5                                                        0.011494  \n",
       " quiz on Leedy Ch 4                                           0.011494  \n",
       "perk\\nEvaluation                                              0.011494  \n",
       "editors                                                       0.011494  \n",
       "com                                                           0.011494  \n",
       "elsevier                                                      0.011494  \n",
       "www                                                           0.011494  \n",
       " Publication Ethics from Elsevier web site\\n\\nh...            0.011494  \n",
       " Sections of Leedy Ch 4                                       0.011494  \n",
       "\\nReading                                                     0.011494  \n",
       " Research Ethics                                              0.011494  \n",
       " Confounding Variables                                        0.011494  \n",
       " Sources of Bias                                              0.011494  \n",
       "\\nSampling                                                    0.011494  \n",
       " Experimental Design                                          0.011494  \n",
       " Quantitative and Qualitative Research                        0.011494  \n",
       " The Research Process                                         0.011494  \n",
       "\\n\\n4                                                         0.011494  \n",
       " List and discuss Strengths and Weaknesses of t...            0.011494  \n",
       "s suggestions                                                 0.011494  \n",
       " according to the Baylor Sections and identify ...            0.011494  \n",
       "from your Literature\\nReview                                  0.011494  \n",
       " Class presentation of an analysis of one paper               0.011494  \n",
       " 4 Bates and 3 Borja pdfs                                     0.011494  \n",
       " Writing a Manuscript for Publication\\nReading                0.011494  \n",
       "\\n\\n3                                                         0.011494  \n",
       "MS\\ntopic due in 4 weeks                                      0.011494  \n",
       " on your PhD                                                  0.011494  \n",
       " 25 references                                                0.011494  \n",
       "5 pages                                                       0.011494  \n",
       " Literature review report                                     0.011494  \n",
       " Skim Leedy Ch3                                               0.011494  \n",
       "\\n\\nReading                                                   0.011494  \n",
       "by Chris Magee                                                0.011494  \n",
       "\\n\\nTraining on Reference Manager Software Zotero             0.011494  \n",
       "by Theresa Calcagno                                           0.011494  \n",
       " How to Do a Literature Review\\n\\nTraining on L...            0.011494  \n",
       " 2\\n\\n2                                                       0.011494  \n",
       " quiz on Leedy Chapters                                       0.011494  \n",
       " in class                                                     0.011494  \n",
       "pdfs                                                          0.011494  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing functions for sentences or phrases\n",
    "path = os.getcwd()\n",
    "sentence_count, size = get_sentence_counter(path)\n",
    "df_sentence = make_df(sentence_count.most_common()[:-n-1:-1], size,\n",
    "             ind_name='Least Commonly\\n Occuring Sentences') # finds the least commonly occuring sentences\n",
    "df_sentence.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing functions for sentences or phrases\n",
    "path = os.getcwd()\n",
    "pages = combine_pages(path) # combining every text file in folder\n",
    "word_count, size = get_text_counter(pages, avoid, word_pattern) # returns tokens freqz and count of tokens\n",
    "n = 1000 # 1000 of least commonly occuring words\n",
    "# least commonly occuring words are chosen with the assumption that a syllabus will only mention\n",
    "# technical terms once or twice while other words like 'students' and so on could be redundant\n",
    "df_word = make_df(word_count.most_common()[:-n-1:-1], size,\n",
    "             ind_name='Least Commonly\\n Occuring Words') # finds the least commonly occuring words\n",
    "df_word.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['proposal', 'page', 'submit', 'reviews', 'submitted', 'discussions', 'participation', 'determined', 'grade', 'section', 'study', 'simulated', 'participate', 'grants', 'reader', 'or', 'reviewer', 'sites', 'reviewing', 'r', 'linked', 'completion', 'ch11', 'procedures', 'statistical', 'analyses', 'perk', 'editors', 'com', 'www', 'https', 'site', 'variables', 'confounding', 'bias', 'sources', 'sampling', 'design', 'experimental', 'qualitative', 'weaknesses', 'strengths', 'list', 'suggestions', 's', 'not', 'does', 'it', 'corresponds', 'identify', 'according', 'one', 'presentation', 'borja', 'bates', 'manuscript', 'weeks', 'due', 'ms', 'phd', 'references', 'pages', 'report', 'ch3', 'skim', 'magee', 'chris', 'zotero', 'software', 'manager', 'reference', 'calcagno', 'theresa', 'searches', 'databases', 'library', 'do', 'how', 'chapters', 'articles', 'present', 'supervisor', 'with', 'thesis', 'ormrod', 'introduction', 'red', 'are', 'assignments', 'evaluations', 'content', 'presentations', 'poster', 'oral', 'effectively', 'communicate', 'as', 'such', 'organizations', 'funding', 'manuscripts', 'scientific', 'include', 'acquire', 'skills', 'specific', 'conclusions', 'drawing', 'interpreting', 'analyzing', 'collecting', 'plan', 'making', 'hypotheses', 'formulating', 'goal', 'articulating', 'search', 'performing', 'problem', 'important', 'identifying', 'understand', 'outcomes', 'learning', 'objectives', 'goals', 'be', 'you', 'web', 'elsevier', 'quantitative', 'where', 'sections', 'baylor', 'from', 'paper', 'training', 'class', 'analysis', 'discuss', 'nsf', 'write', 'data', 'will', 'by', 'quiz', 'pdfs', 'topic', 'nih', 'publication', 'ability', 'develop', 'ethics', 'proposals', 'process', 'course', 'review', 'ch', 'writing', 'an', 'evaluation', 'for', 'literature', 'assignment', 'reading', 'your', 'leedy', 'in', 'to', 'on', 'a', 'of', 'the', 'research', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Just showing a complete list of the words tokenized\n",
    "n = 1000\n",
    "print([x[0] for x in word_count.most_common()[:-n-1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Your grade will be determined by your participation in discussions and submitted\\nreviews', ' You will be a reviewer or reader for 4 grants and participate in a simulated NIH\\nstudy section', '\\n\\nAssignment', ' NSF and NIH pdfs and web sites', ' Writing and Reviewing Research Proposals\\nReading ', '\\n\\n6', ' quiz on Leedy Ch 11', ' Leedy Ch11 and completion of the Linked In course on R\\n\\nEvaluation', ' Quantitative Analyses and Review of Statistical Procedures\\nReading ', '7\\n\\n5', ' quiz on Leedy Ch 4', 'perk\\nEvaluation', 'editors', 'com', 'elsevier', 'www', ' Publication Ethics from Elsevier web site\\n\\nhttps', ' Sections of Leedy Ch 4', '\\nReading ', ' Research Ethics', ' Confounding Variables', ' Sources of Bias', '\\nSampling', ' Experimental Design', ' Quantitative and Qualitative Research', ' The Research Process ', '\\n\\n4', ' List and discuss Strengths and Weaknesses of the Writing', 's suggestions', ' according to the Baylor Sections and identify where the paper corresponds and where it\\ndoes not to Baylor', 'from your Literature\\nReview', ' Class presentation of an analysis of one paper ', ' 4 Bates and 3 Borja pdfs', ' Writing a Manuscript for Publication\\nReading ', '\\n\\n3', 'MS\\ntopic due in 4 weeks', ' on your PhD', ' 25 references', '5 pages', ' Literature review report ', ' Skim Leedy Ch3', '\\n\\nReading ', 'by Chris Magee', '\\n\\nTraining on Reference Manager Software Zotero ', 'by Theresa Calcagno', ' How to Do a Literature Review\\n\\nTraining on Library databases for literature searches ', ' 2\\n\\n2', ' quiz on Leedy Chapters ', ' in class', 'pdfs', ' Discuss your thesis topic with your supervisor and present\\nan analysis of 2 articles ', ' 2', ' Ormrod Ch 1 ', ' Leedy ', ' An Introduction\\n\\nReading ', ' The Research Process', 'assignments are in red', 'evaluations', '\\n\\nCourse Content ', ' Develop the ability to communicate effectively in oral and poster presentations', ' Develop the ability to write research proposals for funding organizations such as NSF\\nand NIH\\n\\n4', ' Develop the ability to write scientific manuscripts for publication\\n\\n3', ' Specific skills to acquire include\\n\\n2', ' research ethics', ' drawing\\nconclusions', ' interpreting data', ' collecting and analyzing data', '\\nwriting research proposals', ' making a research plan', ' formulating hypotheses', ' articulating a goal', ' performing a\\nliterature search', ' Understand the research process of identifying an important problem', 'Learning Outcomes', 'Course Goals and Objectives', ' ', '\\n\\n1', '\\n\\nEvaluation', 'Assignment ']\n"
     ]
    }
   ],
   "source": [
    "# Just showing a complete list of the words tokenized\n",
    "n = 1000\n",
    "print([x[0] for x in sentence_count.most_common()[:-n-1:-1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Installing necessary dependencies\n",
    "import sys\n",
    "# !conda install --yes --prefix {sys.prefix} PIL\n",
    "# # !{sys.executable} -m pip install ____\n",
    "\n",
    "\n",
    "# !{sys.executable} -m pip install pytesseract\n",
    "# !{sys.executable} -m pip install pillow\n",
    "# !{sys.executable} -m pip install python-poppler\n",
    "# !{sys.executable} -m pip install opencv-python\n",
    "# !{sys.executable} -m pip install --upgrade pip\n",
    "# !conda install -c conda-forge poppler\n",
    "\n",
    "# !{sys.executable} -m pip install scipy\n",
    "# !{sys.executable} -m pip install sklearn\n",
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install nltk\n",
    "# !{sys.executable} -m pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyPDF2\n",
    "# import re\n",
    "# import win32com.client\n",
    "# from win32com.client.dynamic import Dispatch\n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "# import pandas as pd\n",
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "# import numpy as np\n",
    "import pytesseract\n",
    "import os\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FUnction to extract words from a pdf and save it in as a text file\n",
    "\n",
    "def pdf_2_txt(path):          \n",
    "    for file in os.listdir(path): # goes through the folders'\n",
    "        if file.endswith('.pdf'): # finds pdfs\n",
    "            p = os.path.join(path,file) # grabs path of pdf\n",
    "            pdfpages = convert_from_path(p, 500) # Converting each page in the pdf into image file with 500 dpi\n",
    "            image_counter = 1\n",
    "            for page in pdfpages:\n",
    "                filename = f\"page_{image_counter}.jpg\"\n",
    "                # Save the image of the page in system\n",
    "                page.save(filename, 'JPEG')\n",
    "                image_counter+=1\n",
    "                text = str(((pytesseract.image_to_string(Image.open(filename))))) # takes image and process into text\n",
    "                text = text.replace('-\\n', '') # some text processing\n",
    "                # delete image\n",
    "                os.remove(filename)\n",
    "\n",
    "                filename = filename.replace('.jpg','') # Naming\n",
    "                with open(f'{filename}.txt', 'w+') as f: # Opens a text file and writes the characters onto\n",
    "                    print(text, file=f)\n",
    "                    print(f'Page_{image_counter}.txt has been saved into {os.getcwd()}')\n",
    "                \n",
    "    return\n",
    "                    \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page_2.txt has been saved into C:\\Users\\15713\\Desktop\n",
      "Page_3.txt has been saved into C:\\Users\\15713\\Desktop\n",
      "Page_4.txt has been saved into C:\\Users\\15713\\Desktop\n",
      "Page_5.txt has been saved into C:\\Users\\15713\\Desktop\n",
      "Page_6.txt has been saved into C:\\Users\\15713\\Desktop\n",
      "Page_7.txt has been saved into C:\\Users\\15713\\Desktop\n",
      "Page_8.txt has been saved into C:\\Users\\15713\\Desktop\n",
      "Page_9.txt has been saved into C:\\Users\\15713\\Desktop\n",
      "Page_10.txt has been saved into C:\\Users\\15713\\Desktop\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# path = os.path.join(os.getcwd(),'syllabus.pdf')\n",
    "path = os.getcwd()\n",
    "pdf_2_txt(path)\n",
    "# pages = convert_from_path(path, 500) \n",
    "# for page in pages:\n",
    "#     page.save('test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, I try to do some text processing and tokenization to see word frequencies\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import copy\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from __future__ import division\n",
    "import matplotlib\n",
    "\n",
    "# % matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\15713\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # downloads all stopwords to avoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pages(path):\n",
    "    # Combining pages\n",
    "    pages = ''\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith('.txt'):\n",
    "            with codecs.open(file, 'r') as f: \n",
    "                page = f.read()\n",
    "                pages+=page\n",
    "    return pages\n",
    "\n",
    "# pages = combine_pages(path)\n",
    "# pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = pages\n",
    "# tokens_P = WordPunctTokenizer().tokenize(PorterStemmer().stem(text))\n",
    "# tokens = WordPunctTokenizer().tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a list of unecessary words like 'the', 'or',... to avoid\n",
    "avoid = stopwords.words('english')\n",
    "avoid.append('edu') # adding our own word to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pattern = re.compile(\"\\w+$\") # finds \\w: all word characters, $: end of string character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_counter(text, avoid, word_pattern):\n",
    "#     tokens = WordPunctTokenizer().tokenize(PorterStemmer().stem(text))\n",
    "    tokens = WordPunctTokenizer().tokenize(text)\n",
    "    tokens = list(map(lambda x: x.lower(), tokens)) # converting everything to lower-case\n",
    "    # cleaning up tokens by getting rid of stopwords and finding word characters\n",
    "    tokens = [token for token in tokens if re.match(word_pattern, token) and token not in avoid] \n",
    "    # collections.counter() returns the tokens and their frequency as a dictionary ({freq: token})\n",
    "    return collections.Counter(tokens), len(tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(counter, size):\n",
    "    abs_freq = np.array([x[1] for x in counter]) # the absolute freqz is found from the collections.counter()\n",
    "    rel_freq = abs_freq/size # relative freqz is calculated by dividing abs_freqz by the size\n",
    "    index = [x[0] for x in counter] # the tokens are used as an index \n",
    "    # creating the dataframe\n",
    "    df = pd.DataFrame(data=np.array([abs_freq, rel_freq]).T, \n",
    "                      index=index,\n",
    "                      columns=[\"Absolute frequency\",\"Relative frequency\"])\n",
    "    # Naming the index title\n",
    "    df.index.name = \"Least Common Words\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "pages = combine_pages(path) # combining every text file in folder\n",
    "word_count, size = get_text_counter(pages, avoid, word_pattern) # returns tokens freqz and count of tokens\n",
    "n = 1000 # 1000 of least commonly occuring words\n",
    "# least commonly occuring words are chosen with the assumption that a syllabus will only mention\n",
    "# technical terms once or twice while other words like 'students' and so on could be redundant\n",
    "df = make_df(word_count.most_common()[:-n-1:-1], size) # finds the least commonly occuring words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Absolute frequency</th>\n",
       "      <th>Relative frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Least Common Words</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ittp</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4111</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hot</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tip</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anonymous</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solvers</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reporting</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>call</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>facility</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urgent</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospital</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nurse</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closed</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campuses</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>william</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prince</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arlington</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fairfax</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>circumstances</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assist</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challenging</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recovery</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collegiate</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>substance</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>areas</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>programming</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consultations</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>titleix</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emailing</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8730</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calling</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seek</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychology</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confidentially</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speak</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stalking</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assault</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disclosures</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employee</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Absolute frequency  Relative frequency\n",
       "Least Common Words                                        \n",
       "ittp                               1.0            0.000478\n",
       "4111                               1.0            0.000478\n",
       "hot                                1.0            0.000478\n",
       "tip                                1.0            0.000478\n",
       "anonymous                          1.0            0.000478\n",
       "solvers                            1.0            0.000478\n",
       "reporting                          1.0            0.000478\n",
       "2810                               1.0            0.000478\n",
       "call                               1.0            0.000478\n",
       "facility                           1.0            0.000478\n",
       "urgent                             1.0            0.000478\n",
       "hospital                           1.0            0.000478\n",
       "2831                               1.0            0.000478\n",
       "nurse                              1.0            0.000478\n",
       "closed                             1.0            0.000478\n",
       "medical                            1.0            0.000478\n",
       "campuses                           1.0            0.000478\n",
       "william                            1.0            0.000478\n",
       "prince                             1.0            0.000478\n",
       "arlington                          1.0            0.000478\n",
       "fairfax                            1.0            0.000478\n",
       "circumstances                      1.0            0.000478\n",
       "3686                               1.0            0.000478\n",
       "3200                               1.0            0.000478\n",
       "assist                             1.0            0.000478\n",
       "challenging                        1.0            0.000478\n",
       "recovery                           1.0            0.000478\n",
       "collegiate                         1.0            0.000478\n",
       "substance                          1.0            0.000478\n",
       "well                               1.0            0.000478\n",
       "financial                          1.0            0.000478\n",
       "areas                              1.0            0.000478\n",
       "programming                        1.0            0.000478\n",
       "consultations                      1.0            0.000478\n",
       "titleix                            1.0            0.000478\n",
       "emailing                           1.0            0.000478\n",
       "8730                               1.0            0.000478\n",
       "calling                            1.0            0.000478\n",
       "seek                               1.0            0.000478\n",
       "psychology                         1.0            0.000478\n",
       "hr                                 1.0            0.000478\n",
       "24                                 1.0            0.000478\n",
       "confidentially                     1.0            0.000478\n",
       "speak                              1.0            0.000478\n",
       "1202                               1.0            0.000478\n",
       "per                                1.0            0.000478\n",
       "stalking                           1.0            0.000478\n",
       "assault                            1.0            0.000478\n",
       "disclosures                        1.0            0.000478\n",
       "employee                           1.0            0.000478"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sunday', '45am', '45pm', 'semester', 'come', 'requests', 'missing', 'entertain', 'returned', 'report', 'grades', 'cyse220_cw02_poppinsmary', 'cyse220_hwk02_doejohn', 'firstname', 'cyse220_assigmenttypeandweek_lastname', 'convention', 'named', 'files', 'explained', 'challenge', 'quiz', 'test', 'prior', 'disclosed', 'amount', 'conducted', 'challenges', 'tests', 'day', 'usually', 'end', '50', '70', 'max', 'earn', 'submit', 'handed', 'beginning', 'present', 'otherwise', 'stated', 'unless', '100', 'types', 'three', 'hard', 'shows', 'consistently', 'past', 'subject', 'understanding', 'insight', 'gain', 'detail', 'sometimes', 'date', 'follows', 'structure', '05', 'pack', 'service', 'v', '1259012051', '1259012050', '2012', 'ii', '0073529271', '0073529273', '848', '2009', 'january', '0073534870', '2010', 'also', 'encouraged', '0073398068', '0073398063', '45s', '928', '2013', 'march', 'textbook', 'friday', 'cosine', 'sine', 'blocks', 'variable', 'state', 'diagram', 'block', 'asynchronous', 'de', 'tbd', '12', 'break', 'interpolation', 'fitting', 'curve', 'polynomials', 'chapter', 'library', 'built', 'graphs', 'formatting', 'plotting', 'commands', 'command', 'fprintf', 'scripts', 'writing', 'class3', 'environment', 'outline', 'committee', 'full', 'oai', 'integrity', '41', 'place', '4113', 'available', 'followed', 'strictly', 'religious_calendar', 'ulife', 'holidays', 'calendar', 'ahead', 'planning', 'responsible', 'events', 'example', 'common', 'one', 'observances', 'turned', 'awarded', 'document', 'accordance', 'credit', 'reduced', 'receive', 'justified', 'properly', 'presented', 'ideas', 'hands', 'practical', 'intended', 'write', 'interact', 'permitted', 'coordinate', 'responsibility', 'participation', 'event', 'change', 'flexibility', 'work', 'electronically', 'deliverables', 'almost', 'exceptions', 'call', 'incoming', 'answer', 'extreme', 'find', 'mode', 'silent', 'pager', 'tablet', 'phone', 'cell', 'going', 'activity', 'disrupting', 'avoid', 'best', 'enter', 'allowed', 'miss', 'essential', 'attendance', 'days', 'snow', 'associated', 'online', 'attend', 'updates', 'area', 'announcements', 'emergencies', 'remotely', 'sessions', 'whole', 'updated', 'counselor', 'hear', 'waiting', 'center', 'contacted', 'possible', 'soon', 'needs', 'accommodation', 'know', 'let', 'b', 'determine', 'ods', '2474', '4205', 'rm', 'sub', 'documentation', 'performance', 'affect', 'condition', 'learning', 'documented', 'complete', 'assistance', 'however', 'connecting', 'problems', 'help', 'try', 'vse', 'labs', 'site', 'web', 'many', 'answers', 'volgenau', 'bottom', 'link', 'instructions', 'mymasonportal', 'https', 'mymason', 'accessible', 'start', 'done', 'communication', 'logistics', '66', '33', 'jan', 'without', 'registration', 'administrative', '9989', '703', 'dr', 'appointment', 'building', '2227', 'hours', 'wednesdays', '160', 'phys', 'math', 'prerequisites', 'studying', 'numerical', 'analytical', 'mechanical', 'power', 'electrical', 'transportation', 'economic', 'descriptions', 'mathematical', 'formulation', 'introduces', '17632', 'crn', 'research', 'operations', 'department', 'phd', 'prof', 'classwork', 'post', 'next', 'always', 'announcement', 'respective', '35', 'grading', 'later', 'february', 'science', 'paperback', 'following', 'pp', 'exam', 'first', 'solver', 'ode45', '18', 'transfer', '28', '23', 'stability', 'parameters', 'response', 'expansion', 'fraction', 'partial', '21', '24', '22', '17', 'examples', 'dynamic', 'snowstorm', 'cancelled', 'see', 'defined', 'navoid', 'catoid', 'content', 'given', 'refer', 'might', 'religious', 'policy', 'late', 'experience', 'student', 'impact', 'scheduled', 'submitted', 'etc', 'leave', 'quizzes', 'well', 'website', 'check', 'collaborate', 'held', 'weather', 'inclement', 'process', 'keep', 'services', 'file', 'sure', 'make', 'may', 'accommodations', 'software', 'provide', 'provided', 'portal', 'able', 'access', 'deadline', '993', 'data', '00', '45', 'mondays', 'higher', 'details', 'using', 'nonlinear', 'continuous', 'discrete', 'behavior', 'solving', 'methods', 'computer', 'including', 'dynamical', 'description', 'pcosta', 'engineering', 'exams', 'engineers', '978', 'dynamics', 'simulink', 'order', '16', 'transform', 'laplace', '15', 'functions', '27', '25', '20', 'catalog', 'policies', 'php', '7', 'posted', 'advance', 'must', 'e', 'activities', 'need', 'general', 'questions', 'computing', 'school', 'resources', 'use', 'expected', '26', 'penalty', 'tuition', 'drop', 'contact', 'p', 'office', 'hall', 'innovation', 'linear', 'edition', 'hill', 'mcgraw', 'iii', 'differential', 'review', 'code', '8', 'academic', 'disability', '11', '203', 'time', 'models', 'palm', 'j', 'william', 'midterm', '13', 'equations', '14', '9', 'honor', 'solutions', 'homework', 'please', 'version', '19', 'final', '30', 'room', 'classes', 'modeling', 'points', 'due', '29', 'syllabus', 'paulo', '220', 'cyse', 'isbn', 'page', 'via', 'feb', 'grade', 'matlab', 'spring', 'http', 'george', 'costa', 'c', '5', 'blackboard', 'introduction', 'g', 'assignment', 'students', 'mason', 'system', '1', '10', 'gmu', 'university', 'systems', '6', '2016', 'course', 'assignments', '4', '2', '3', 'class']\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "print([x[0] for x in word_count.most_common()[:-n-1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Absolute frequency</th>\n",
       "      <th>Relative frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Most Common Words</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>past</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consistently</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shows</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>three</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unless</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stated</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>otherwise</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>present</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beginning</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handed</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>submit</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earn</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usually</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tests</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challenges</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conducted</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disclosed</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prior</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quiz</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>challenge</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explained</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>files</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>named</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>convention</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyse220_assigmenttypeandweek_lastname</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>firstname</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyse220_hwk02_doejohn</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyse220_cw02_poppinsmary</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grades</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>report</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>returned</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertain</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>requests</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semester</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45pm</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45am</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunday</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Absolute frequency  Relative frequency\n",
       "Most Common Words                                                            \n",
       "subject                                               1.0             0.00088\n",
       "past                                                  1.0             0.00088\n",
       "consistently                                          1.0             0.00088\n",
       "shows                                                 1.0             0.00088\n",
       "hard                                                  1.0             0.00088\n",
       "three                                                 1.0             0.00088\n",
       "types                                                 1.0             0.00088\n",
       "100                                                   1.0             0.00088\n",
       "unless                                                1.0             0.00088\n",
       "stated                                                1.0             0.00088\n",
       "otherwise                                             1.0             0.00088\n",
       "present                                               1.0             0.00088\n",
       "beginning                                             1.0             0.00088\n",
       "handed                                                1.0             0.00088\n",
       "submit                                                1.0             0.00088\n",
       "earn                                                  1.0             0.00088\n",
       "max                                                   1.0             0.00088\n",
       "70                                                    1.0             0.00088\n",
       "50                                                    1.0             0.00088\n",
       "end                                                   1.0             0.00088\n",
       "usually                                               1.0             0.00088\n",
       "day                                                   1.0             0.00088\n",
       "tests                                                 1.0             0.00088\n",
       "challenges                                            1.0             0.00088\n",
       "conducted                                             1.0             0.00088\n",
       "amount                                                1.0             0.00088\n",
       "disclosed                                             1.0             0.00088\n",
       "prior                                                 1.0             0.00088\n",
       "test                                                  1.0             0.00088\n",
       "quiz                                                  1.0             0.00088\n",
       "challenge                                             1.0             0.00088\n",
       "explained                                             1.0             0.00088\n",
       "files                                                 1.0             0.00088\n",
       "named                                                 1.0             0.00088\n",
       "convention                                            1.0             0.00088\n",
       "cyse220_assigmenttypeandweek_lastname                 1.0             0.00088\n",
       "firstname                                             1.0             0.00088\n",
       "cyse220_hwk02_doejohn                                 1.0             0.00088\n",
       "cyse220_cw02_poppinsmary                              1.0             0.00088\n",
       "grades                                                1.0             0.00088\n",
       "report                                                1.0             0.00088\n",
       "returned                                              1.0             0.00088\n",
       "entertain                                             1.0             0.00088\n",
       "missing                                               1.0             0.00088\n",
       "requests                                              1.0             0.00088\n",
       "come                                                  1.0             0.00088\n",
       "semester                                              1.0             0.00088\n",
       "45pm                                                  1.0             0.00088\n",
       "45am                                                  1.0             0.00088\n",
       "sunday                                                1.0             0.00088"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(50)\n",
    "# word_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

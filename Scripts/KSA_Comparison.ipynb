{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, I try to do some text processing and tokenization to see word frequencies\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import copy\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from __future__ import division\n",
    "# import matplotlib\n",
    "import json\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "with open('Master_indeed_dict.json') as json_file:\n",
    "    Master_indeed_dict = json.load(json_file)\n",
    "with open('USAJ_tokenized_dict.json') as json_file:\n",
    "    USAJ_tokenized_dict = json.load(json_file)\n",
    "with open('syllabi_NICCS.json') as json_file:\n",
    "    syllabi_NICCS_dict = json.load(json_file)    \n",
    "with open('Certs.json') as json_file:\n",
    "    Certs_dict = json.load(json_file)  \n",
    "with open('similar.json') as json_file:\n",
    "    similar = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity   \n",
    "\n",
    "def vectorize(list1):\n",
    "    vecs_1 = []\n",
    "    for sent in list1:\n",
    "        vecs_1.append(model.encode(sent))\n",
    "    return vecs_1\n",
    "\n",
    "def Alignment(list1, list2):\n",
    "    alignment = {}\n",
    "    tmp = []\n",
    "    for sentence in list1:\n",
    "        # if type(sentence) is list:\n",
    "        #     print(\"list\")\n",
    "        # if type(sentence) is str:\n",
    "        sim = cosine_similarity([sentence], list2[:])\n",
    "        # mean C_s of 1 sentence in list1 vs all sentences in list2\n",
    "        tmp.append(np.mean(sim))\n",
    "\n",
    "        alignment['cos_all'] = tmp\n",
    "        alignment['cos_mean'] = np.mean(tmp) if len(tmp)>=1 else 0\n",
    "        tmp_ = []\n",
    "        for val in tmp:\n",
    "            tmp_.append(1-np.arccos(val)/np.pi) #converts into angle and normalize [-1 1]\n",
    "        alignment['angle'] = tmp_ \n",
    "        alignment['angle_mean'] = np.mean(tmp_) if len(tmp_)>=1 else 0\n",
    "        \n",
    "    return alignment\n",
    "    \n",
    "\n",
    "def Compare(dict1, dict2, vec=False, baseline=False):\n",
    "   Sim = {}\n",
    "   if baseline:\n",
    "       for K in dict1.keys():\n",
    "            Sim[K] = {}\n",
    "            vec_1 = dict1[K]['all_vec'] if vec else vectorize(dict1[K]['All'])\n",
    "            # for k in dict2.keys():\n",
    "            #     vec_2 = dict2[k]['all_vec'] if vec else vectorize(dict2[k]['All'])\n",
    "            try:\n",
    "                Sim[K][K] = Alignment(vec_1, vec_1)\n",
    "            except:\n",
    "                    pass\n",
    "   else:\n",
    "       for K in dict1.keys():\n",
    "        Sim[K] = {}\n",
    "        vec_1 = dict1[K]['all_vec'] if vec else vectorize(dict1[K]['All'][:2])\n",
    "        for k in dict2.keys():\n",
    "            vec_2 = dict2[k]['all_vec'] if vec else vectorize(dict2[k]['All'][:2])\n",
    "            try:\n",
    "                    Sim[K][k] = Alignment(vec_1, vec_2)\n",
    "            except:\n",
    "                    pass\n",
    "        \n",
    "   return Sim\n",
    "   \n",
    "   \n",
    "\n",
    "def Score(baseline, mean):\n",
    "    return 1-np.abs((baseline-mean)/mean)\n",
    "\n",
    "def Calculate_Score(dict1, baseline_dict, key=False):\n",
    "    list1 = []\n",
    "    for K in dict1.keys():\n",
    "        baseline = baseline_dict[K][K]['cos_mean']\n",
    "        for k in dict1[K].keys():\n",
    "            if not key:\n",
    "                mean = dict1[K][k]['cos_mean']\n",
    "                list1.append(Score(baseline,mean))\n",
    "            else:\n",
    "                baseline = baseline_dict[K][K][key]\n",
    "                mean = dict1[K][k][key]\n",
    "                list1.append(Score(baseline,mean))\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## HERE I want to vectorize each sentence while preserving the data structure (instead of a sentence, we will have a vector/list)\n",
    "syllabi_niccs_vecs = {}\n",
    "for WR in syllabi_NICCS_dict.keys():\n",
    "    all_vec = model.encode(syllabi_NICCS_dict[WR]['All'])\n",
    "    k_vec = model.encode(syllabi_NICCS_dict[WR]['K'])\n",
    "    s_vec = model.encode(syllabi_NICCS_dict[WR]['S'])\n",
    "    a_vec = model.encode(syllabi_NICCS_dict[WR]['A'])\n",
    "    \n",
    "    syllabi_niccs_vecs[WR] = {'all_vec': all_vec,\n",
    "                            'K': k_vec, \n",
    "                             'S': s_vec, \n",
    "                             'A': a_vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# USAJobs\n",
    "## HERE I want to vectorize each sentence while preserving the data structure (instead of a sentence, we will have a vector/list)\n",
    "usaJ_vecs = {}\n",
    "for job in USAJ_tokenized_dict.keys():\n",
    "    all_vec = model.encode(USAJ_tokenized_dict[job]['All'])\n",
    "    k_vec = model.encode(USAJ_tokenized_dict[job]['K'])\n",
    "    s_vec = model.encode(USAJ_tokenized_dict[job]['S'])\n",
    "    a_vec = model.encode(USAJ_tokenized_dict[job]['A'])\n",
    "    \n",
    "    usaJ_vecs[job] = {'all_vec': all_vec, 'K': k_vec, \n",
    "                               'S': s_vec, \n",
    "                               'A': a_vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## HERE I want to vectorize each sentence while preserving the data structure (instead of a sentence, we will have a vector/list)\n",
    "Indeed_vecs = {}\n",
    "for job in Master_indeed_dict.keys():\n",
    "    all_vec = model.encode(Master_indeed_dict[job]['All'])\n",
    "    k_vec = model.encode(Master_indeed_dict[job]['K'])\n",
    "    s_vec = model.encode(Master_indeed_dict[job]['S'])\n",
    "    a_vec = model.encode(Master_indeed_dict[job]['A'])\n",
    "    \n",
    "    Indeed_vecs[job] = {'all_vec': all_vec, 'K': k_vec, \n",
    "                               'S': s_vec, \n",
    "                               'A': a_vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## HERE I want to vectorize each sentence while preserving the data structure (instead of a sentence, we will have a vector/list)\n",
    "Certs_vecs = {}\n",
    "for cert in Certs_dict.keys():\n",
    "    all_vec = model.encode(Certs_dict[cert]['All'])\n",
    "    k_vec = model.encode(Certs_dict[cert]['K'])\n",
    "    s_vec = model.encode(Certs_dict[cert]['S'])\n",
    "    a_vec = model.encode(Certs_dict[cert]['A'])\n",
    "    \n",
    "    Certs_vecs[cert] = {'all_vec': all_vec,'K': k_vec, \n",
    "                        'S': s_vec, \n",
    "                        'A': a_vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### New Approach:\n",
    "# # Well, this approach is not really new since it was the obvious thing to do. My first initial try at comparing\n",
    "# # similarities between job postings and the other datasets, I first tried to categorize the job titles under one of the \n",
    "# # 52 work roles defined by NIST; I even tried to group them manually by going through the job titles and descriptions to see\n",
    "# # where they fit. Neverthless, due to my limited knowledge about cyber-security jobs, job titles that list two different work \n",
    "# # roles separated by a '/' and the tidious manual work, it seemed to be impossible.\n",
    "# # Therefore, I just decided to compare each workrole to every single job posting.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the baselines\n",
    "baseline_in = Compare(Indeed_vecs, Indeed_vecs, vec=True,baseline=True)\n",
    "baseline_us = Compare(usaJ_vecs, usaJ_vecs, vec=True,baseline=True)\n",
    "baseline_sy = Compare(syllabi_niccs_vecs, syllabi_niccs_vecs, vec=True,baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syllabi VS USAJobas\n",
    "\n",
    "a = Compare(usaJ_vecs, syllabi_niccs_vecs, vec=True)\n",
    "a_1 = Compare(syllabi_niccs_vecs, usaJ_vecs, vec=True)\n",
    "\n",
    "US_Sy_cos = Calculate_Score(a, baseline_us)\n",
    "# print(np.mean(US_Sy_cos))\n",
    "SY_US_cos = Calculate_Score(a_1, baseline_sy)\n",
    "print(np.mean(SY_US_cos+US_Sy_cos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indeed Vs Syllabi\n",
    "\n",
    "b = Compare(Indeed_vecs, syllabi_niccs_vecs, vec=True)\n",
    "b_1 = Compare(syllabi_niccs_vecs, Indeed_vecs, vec=True)\n",
    "\n",
    "I_Sy_cos = Calculate_Score(b, baseline_in)\n",
    "# print(np.mean(I_Sy_cos))\n",
    "SY_I_cos = Calculate_Score(b_1, baseline_sy)\n",
    "print(np.mean(SY_I_cos+I_Sy_cos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indeed VS USAJobs\n",
    "\n",
    "c = Compare(Indeed_vecs, usaJ_vecs, vec=True)\n",
    "c_2 = Compare(usaJ_vecs, Indeed_vecs, vec=True)\n",
    "\n",
    "I_US_cos = Calculate_Score(c, baseline_in)\n",
    "US_I_cos = Calculate_Score(c_2, baseline_us)\n",
    "\n",
    "print(np.mean(US_I_cos+I_US_cos))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f34e902187b2aa61a959272893b6024f9571e82bc13755d09190a322a23c44e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('HCP_similarity-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

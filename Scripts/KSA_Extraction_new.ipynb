{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which environment we are working in\n",
    "# !conda info --envs # check which env we are in\n",
    "# !conda activate HCP-env # Activate the right environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import copy\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\15713\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # downloads all stopwords to avoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a list of unecessary words like 'the', 'or',... to avoid\n",
    "additional_words = ['edu', 'transcript', 'hour', 'program', 'experience', 'work', 'must', 'provide']\n",
    "with open('add_stopwords.txt') as f:\n",
    "    lines = f.readlines()\n",
    "add_stopwords = lines[0].split(',')\n",
    "\n",
    "avoid = stopwords.words('english') + add_stopwords + additional_words# adding our own word to the list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\15713\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # downloads the tokenizer we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a text file into sentences or phrases\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def get_sentence_counter(path=None, text=None):\n",
    "    if path:\n",
    "        path = os.getcwd()\n",
    "        for file in os.listdir():\n",
    "            if file.endswith('.txt'):\n",
    "                with codecs.open(file, 'r') as f: \n",
    "                    text = f.read()\n",
    "    else:\n",
    "        text = text\n",
    "    # raw_tokens = tokenizer.tokenize(text)\n",
    "    # tokens = []\n",
    "    # for token in raw_tokens:\n",
    "    #     if '\\n' in token:\n",
    "    #         tmp = token.split('\\n')\n",
    "    #         for sentence in tmp:\n",
    "    #             tokens.append(sentence)\n",
    "\n",
    "    # # tmp = [[x for x in token.split('\\n')] for token in raw_sentences]\n",
    "\n",
    "    return tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a text file into words\n",
    "def remove_stopwords(text, avoid):\n",
    "#     tokens = WordPunctTokenizer().tokenize(PorterStemmer().stem(text))\n",
    "    try:\n",
    "        tokens = WordPunctTokenizer().tokenize(text)\n",
    "        tokens = list(map(lambda x: x.lower(), tokens)) # converting everything to lower-case\n",
    "        \n",
    "        # cleaning up tokens by getting rid of stopwords and finding word characters\n",
    "        tokens = [token for token in tokens if token not in avoid]\n",
    "\n",
    "        sentence = ''\n",
    "        for token in tokens:\n",
    "            sentence = sentence + token + ' '\n",
    "    except:\n",
    "        sentence= ''\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "# INDEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with 'Master_Indeed_Dataset'\n",
    "path = r'C:\\Users\\15713\\Desktop\\Datasets\\csv_xlsx\\Master_Indeed_Dataset.xlsx'\n",
    "df = pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Links</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JUNIOR PENETRATION TESTER</td>\n",
       "      <td>Elevate</td>\n",
       "      <td>Remote</td>\n",
       "      <td>https://indeed.com/viewjob?jk=0a12694d75919f0c</td>\n",
       "      <td>$35 - $50 an hour</td>\n",
       "      <td>JUNIOR PENETRATION TESTER\\nwww.elevateconsult....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior Penetration Tester USA Remote</td>\n",
       "      <td>BreachLock</td>\n",
       "      <td>Floridaâ€¢Remote</td>\n",
       "      <td>https://indeed.com/viewjob?jk=1c98609eab9789cc</td>\n",
       "      <td>None</td>\n",
       "      <td>The Penetration Testing professional should ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title     Company          Location  \\\n",
       "0             JUNIOR PENETRATION TESTER     Elevate            Remote   \n",
       "1  Junior Penetration Tester USA Remote  BreachLock  Floridaâ€¢Remote   \n",
       "\n",
       "                                            Links             Salary  \\\n",
       "0  https://indeed.com/viewjob?jk=0a12694d75919f0c  $35 - $50 an hour   \n",
       "1  https://indeed.com/viewjob?jk=1c98609eab9789cc               None   \n",
       "\n",
       "                                         Description  \n",
       "0  JUNIOR PENETRATION TESTER\\nwww.elevateconsult....  \n",
       "1  The Penetration Testing professional should ha...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with title as index and description as an attribute\n",
    "title, description = df['Title'].to_list(), df['Description'].to_list()\n",
    "indeed_df = pd.DataFrame(data={\"Title\": title,\n",
    "                               \"Description\": description} \n",
    "                         )\n",
    "# removing rows with 'None' for their Job-Title\n",
    "# indeed_df = indeed_df[indeed_df.Title != 'None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JUNIOR PENETRATION TESTER</td>\n",
       "      <td>JUNIOR PENETRATION TESTER\\nwww.elevateconsult....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior Penetration Tester USA Remote</td>\n",
       "      <td>The Penetration Testing professional should ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red Team - Penetration Tester</td>\n",
       "      <td>Who are we?\\nBreachLock is a security startup ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title  \\\n",
       "0             JUNIOR PENETRATION TESTER   \n",
       "1  Junior Penetration Tester USA Remote   \n",
       "2         Red Team - Penetration Tester   \n",
       "\n",
       "                                         Description  \n",
       "0  JUNIOR PENETRATION TESTER\\nwww.elevateconsult....  \n",
       "1  The Penetration Testing professional should ha...  \n",
       "2  Who are we?\\nBreachLock is a security startup ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeed_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the above dataframe into dictionary for iteration {title: description}\n",
    "indeed = {}\n",
    "cnt,cnt_=0,0\n",
    "for (t, d) in zip(title, description): # loops through the list title and description\n",
    "    if t in indeed and t!='None': # if the job title already exists, we lump the descriptions together\n",
    "        cnt_+=1\n",
    "        indeed[f'{t}_{cnt_}'] = d\n",
    "    elif t=='None':\n",
    "        cnt+=1\n",
    "        indeed[f'{t}_{cnt}'] = d \n",
    "    else:\n",
    "        indeed[t] = d # sets the title as key and the description as the value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating through the Job's to tokenize into sentences/phrases\n",
    "indeed_tokenized_dict = {}\n",
    "for key in indeed:\n",
    "    # if key != 'None':\n",
    "    text = indeed[key]\n",
    "    # Tokenize into sentence or phrase\n",
    "    sentence_count = get_sentence_counter(text=text)\n",
    "    indeed_tokenized_dict[key] = sentence_count\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting in dataframe with keys/job titles as titles and the values as the tokenized sentences just for visual purposes\n",
    "df_concat = pd.DataFrame()\n",
    "for k,v in indeed_tokenized_dict.items():\n",
    "    # if k != 'None':\n",
    "    df = pd.DataFrame({k: v})\n",
    "    df_concat = pd.concat([df_concat, df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concat.T.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['understand', 'perform', 'execute', 'basic', 'capable', 'conduct', 'responsible', ' able ', ' ability', ' abilities', 'perform', 'certification', ' know', 'known', 'knowledge', 'Knowledge', 'skill', ' able ', ' ability', ' abilities', 'perform']\n"
     ]
    }
   ],
   "source": [
    "# Here I will try to create 'bag of words' that could be used to identify if a sentence is refering to \n",
    "# ability, skill, knowledge or other\n",
    "bag_A = [' able ', ' ability', ' abilities', 'perform']\n",
    "bag_S = ['skill']\n",
    "bag_K = [' know', 'known', 'knowledge', 'Knowledge']\n",
    "bag_C = ['certification']\n",
    "general_bag = ['understand', 'perform','execute', 'basic', 'capable', 'conduct','responsible']\n",
    "storage = general_bag + bag_A+bag_C+bag_K+bag_S+bag_A\n",
    "print(storage)\n",
    "# Helper function to check whether the flag words are present in the sentence\n",
    "def check_word(word_list,sentence):\n",
    "    for word in word_list:\n",
    "        return True if re.findall(word, text.lower()) else False\n",
    "\n",
    "\n",
    "# helper function to extract certs if they are listed out in job descriptions by using a reference list from NIST \n",
    "def find_cert(sentence, cert_list):\n",
    "    for cert in cert_list:\n",
    "        tmp = cert.lower().replace('certified', '')\n",
    "        if tmp in sentence:\n",
    "            return cert\n",
    "# Loading a simplified list of certs found from NIST\n",
    "path = r'C:\\Users\\15713\\Desktop\\Datasets\\csv_xlsx\\cert_list.xlsx' \n",
    "cert_list = pd.read_excel(path, sheet_name='names') \n",
    "cert_list = cert_list['Name'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will sort out the sentences into Ability, skill, knowledge or Certs and save it as a nested dict\n",
    "Master_indeed_dict = {}\n",
    "pattern = '[\\s|^|^\\s][A-Z(][A-Z]+[\\s|)|,]'\n",
    "# I_dict = {}\n",
    "for k in indeed_tokenized_dict.keys():\n",
    "    # print(f'k = {k}')\n",
    "    K,S,A,C,All = [],[],[],[],[]\n",
    "    for sent in indeed_tokenized_dict[k]:\n",
    "        # print(f\"sent ={sent} +'\\n'\")\n",
    "        if check_word(storage,sent):\n",
    "            sentence = remove_stopwords(sent, avoid) # Removing stopwords and reconstructing sentence\n",
    "            # print(f\"sentence ={sentence} +'\\n'\")\n",
    "            All.append(sentence)\n",
    "            # print(f\"All ={All} +'\\n'\")\n",
    "            # break\n",
    "        if check_word(bag_K,sent):\n",
    "            sentence = remove_stopwords(sent, avoid) # Removing stopwords and reconstructing sentence\n",
    "            K.append(sentence)\n",
    "        if check_word(bag_S,sent):\n",
    "            sentence = remove_stopwords(sent, avoid) # Removing stopwords and reconstructing sentence\n",
    "            S.append(sentence)\n",
    "        if check_word(bag_A,sent):\n",
    "            sentence = remove_stopwords(sent, avoid) # Removing stopwords and reconstructing sentence\n",
    "            A.append(sentence)\n",
    "        \n",
    "        # Here, I tried to use regex to find patterns matching a valid Certificate by first\n",
    "        # finding a sentence that contains the flag words\n",
    "        if check_word(bag_C, sent):\n",
    "            # print(f'\\nsentence: {sent}') #printing the sentence to be checked for valid certs\n",
    "            match = re.findall(pattern, sent) #checking words matching our pattern\n",
    "            # print('found certs...')\n",
    "            for x in match:\n",
    "                x = re.sub('\\W', '', x) #removes any non-word characters (space,symbols)\n",
    "                C.append(x) #adding cleaned up string to the certs list\n",
    "                # print(f'-{x}')\n",
    "        # break\n",
    "  \n",
    "    Master_indeed_dict[k] = {'All':All,\n",
    "                             'K':K,\n",
    "                             'S':S,\n",
    "                             'A':A,\n",
    "                             'C':list(set(C))\n",
    "                            }\n",
    "   \n",
    "# Exporting dictionary as a json file for use during sentence vectorization\n",
    "with open('Master_indeed_dict.json', 'w') as f:\n",
    "    json.dump(Master_indeed_dict, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Visualization\n",
    "I_df =  pd.DataFrame.from_dict({(i,j): Master_indeed_dict[i][j] \n",
    "                           for i in Master_indeed_dict.keys() \n",
    "                           for j in Master_indeed_dict[i].keys()},\n",
    "                       orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(JUNIOR PENETRATION TESTER, All)</th>\n",
       "      <td>junior penetration tester www . elevateconsult...</td>\n",
       "      <td>consists self managed , high caliber professio...</td>\n",
       "      <td>value exceptional client , solving coaching cl...</td>\n",
       "      <td>looking motivated , experienced self managed j...</td>\n",
       "      <td>contract hire starts competitive hourly rate e...</td>\n",
       "      <td>elevate looking junior penetration tester ( â ...</td>\n",
       "      <td>junior penetration tester assessments clients ...</td>\n",
       "      <td>ideal candidate think creatively , independent...</td>\n",
       "      <td>seek enjoy learning add skillset quickly probl...</td>\n",
       "      <td>interacts frequently clients , engagement , fi...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(JUNIOR PENETRATION TESTER, K)</th>\n",
       "      <td>junior penetration tester www . elevateconsult...</td>\n",
       "      <td>consists self managed , high caliber professio...</td>\n",
       "      <td>value exceptional client , solving coaching cl...</td>\n",
       "      <td>looking motivated , experienced self managed j...</td>\n",
       "      <td>contract hire starts competitive hourly rate e...</td>\n",
       "      <td>elevate looking junior penetration tester ( â ...</td>\n",
       "      <td>junior penetration tester assessments clients ...</td>\n",
       "      <td>ideal candidate think creatively , independent...</td>\n",
       "      <td>seek enjoy learning add skillset quickly probl...</td>\n",
       "      <td>interacts frequently clients , engagement , fi...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(JUNIOR PENETRATION TESTER, S)</th>\n",
       "      <td>junior penetration tester www . elevateconsult...</td>\n",
       "      <td>consists self managed , high caliber professio...</td>\n",
       "      <td>value exceptional client , solving coaching cl...</td>\n",
       "      <td>looking motivated , experienced self managed j...</td>\n",
       "      <td>contract hire starts competitive hourly rate e...</td>\n",
       "      <td>elevate looking junior penetration tester ( â ...</td>\n",
       "      <td>junior penetration tester assessments clients ...</td>\n",
       "      <td>ideal candidate think creatively , independent...</td>\n",
       "      <td>seek enjoy learning add skillset quickly probl...</td>\n",
       "      <td>interacts frequently clients , engagement , fi...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(JUNIOR PENETRATION TESTER, A)</th>\n",
       "      <td>junior penetration tester www . elevateconsult...</td>\n",
       "      <td>consists self managed , high caliber professio...</td>\n",
       "      <td>value exceptional client , solving coaching cl...</td>\n",
       "      <td>looking motivated , experienced self managed j...</td>\n",
       "      <td>contract hire starts competitive hourly rate e...</td>\n",
       "      <td>elevate looking junior penetration tester ( â ...</td>\n",
       "      <td>junior penetration tester assessments clients ...</td>\n",
       "      <td>ideal candidate think creatively , independent...</td>\n",
       "      <td>seek enjoy learning add skillset quickly probl...</td>\n",
       "      <td>interacts frequently clients , engagement , fi...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(JUNIOR PENETRATION TESTER, C)</th>\n",
       "      <td>PENETRATION</td>\n",
       "      <td>OS</td>\n",
       "      <td>ABOUT</td>\n",
       "      <td>CPT</td>\n",
       "      <td>GPEN</td>\n",
       "      <td>CMWAPT</td>\n",
       "      <td>LPT</td>\n",
       "      <td>CRTOP</td>\n",
       "      <td>TIA</td>\n",
       "      <td>IT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                0    \\\n",
       "(JUNIOR PENETRATION TESTER, All)  junior penetration tester www . elevateconsult...   \n",
       "(JUNIOR PENETRATION TESTER, K)    junior penetration tester www . elevateconsult...   \n",
       "(JUNIOR PENETRATION TESTER, S)    junior penetration tester www . elevateconsult...   \n",
       "(JUNIOR PENETRATION TESTER, A)    junior penetration tester www . elevateconsult...   \n",
       "(JUNIOR PENETRATION TESTER, C)                                          PENETRATION   \n",
       "\n",
       "                                                                                1    \\\n",
       "(JUNIOR PENETRATION TESTER, All)  consists self managed , high caliber professio...   \n",
       "(JUNIOR PENETRATION TESTER, K)    consists self managed , high caliber professio...   \n",
       "(JUNIOR PENETRATION TESTER, S)    consists self managed , high caliber professio...   \n",
       "(JUNIOR PENETRATION TESTER, A)    consists self managed , high caliber professio...   \n",
       "(JUNIOR PENETRATION TESTER, C)                                                   OS   \n",
       "\n",
       "                                                                                2    \\\n",
       "(JUNIOR PENETRATION TESTER, All)  value exceptional client , solving coaching cl...   \n",
       "(JUNIOR PENETRATION TESTER, K)    value exceptional client , solving coaching cl...   \n",
       "(JUNIOR PENETRATION TESTER, S)    value exceptional client , solving coaching cl...   \n",
       "(JUNIOR PENETRATION TESTER, A)    value exceptional client , solving coaching cl...   \n",
       "(JUNIOR PENETRATION TESTER, C)                                                ABOUT   \n",
       "\n",
       "                                                                                3    \\\n",
       "(JUNIOR PENETRATION TESTER, All)  looking motivated , experienced self managed j...   \n",
       "(JUNIOR PENETRATION TESTER, K)    looking motivated , experienced self managed j...   \n",
       "(JUNIOR PENETRATION TESTER, S)    looking motivated , experienced self managed j...   \n",
       "(JUNIOR PENETRATION TESTER, A)    looking motivated , experienced self managed j...   \n",
       "(JUNIOR PENETRATION TESTER, C)                                                  CPT   \n",
       "\n",
       "                                                                                4    \\\n",
       "(JUNIOR PENETRATION TESTER, All)  contract hire starts competitive hourly rate e...   \n",
       "(JUNIOR PENETRATION TESTER, K)    contract hire starts competitive hourly rate e...   \n",
       "(JUNIOR PENETRATION TESTER, S)    contract hire starts competitive hourly rate e...   \n",
       "(JUNIOR PENETRATION TESTER, A)    contract hire starts competitive hourly rate e...   \n",
       "(JUNIOR PENETRATION TESTER, C)                                                 GPEN   \n",
       "\n",
       "                                                                                5    \\\n",
       "(JUNIOR PENETRATION TESTER, All)  elevate looking junior penetration tester ( â ...   \n",
       "(JUNIOR PENETRATION TESTER, K)    elevate looking junior penetration tester ( â ...   \n",
       "(JUNIOR PENETRATION TESTER, S)    elevate looking junior penetration tester ( â ...   \n",
       "(JUNIOR PENETRATION TESTER, A)    elevate looking junior penetration tester ( â ...   \n",
       "(JUNIOR PENETRATION TESTER, C)                                               CMWAPT   \n",
       "\n",
       "                                                                                6    \\\n",
       "(JUNIOR PENETRATION TESTER, All)  junior penetration tester assessments clients ...   \n",
       "(JUNIOR PENETRATION TESTER, K)    junior penetration tester assessments clients ...   \n",
       "(JUNIOR PENETRATION TESTER, S)    junior penetration tester assessments clients ...   \n",
       "(JUNIOR PENETRATION TESTER, A)    junior penetration tester assessments clients ...   \n",
       "(JUNIOR PENETRATION TESTER, C)                                                  LPT   \n",
       "\n",
       "                                                                                7    \\\n",
       "(JUNIOR PENETRATION TESTER, All)  ideal candidate think creatively , independent...   \n",
       "(JUNIOR PENETRATION TESTER, K)    ideal candidate think creatively , independent...   \n",
       "(JUNIOR PENETRATION TESTER, S)    ideal candidate think creatively , independent...   \n",
       "(JUNIOR PENETRATION TESTER, A)    ideal candidate think creatively , independent...   \n",
       "(JUNIOR PENETRATION TESTER, C)                                                CRTOP   \n",
       "\n",
       "                                                                                8    \\\n",
       "(JUNIOR PENETRATION TESTER, All)  seek enjoy learning add skillset quickly probl...   \n",
       "(JUNIOR PENETRATION TESTER, K)    seek enjoy learning add skillset quickly probl...   \n",
       "(JUNIOR PENETRATION TESTER, S)    seek enjoy learning add skillset quickly probl...   \n",
       "(JUNIOR PENETRATION TESTER, A)    seek enjoy learning add skillset quickly probl...   \n",
       "(JUNIOR PENETRATION TESTER, C)                                                  TIA   \n",
       "\n",
       "                                                                                9    \\\n",
       "(JUNIOR PENETRATION TESTER, All)  interacts frequently clients , engagement , fi...   \n",
       "(JUNIOR PENETRATION TESTER, K)    interacts frequently clients , engagement , fi...   \n",
       "(JUNIOR PENETRATION TESTER, S)    interacts frequently clients , engagement , fi...   \n",
       "(JUNIOR PENETRATION TESTER, A)    interacts frequently clients , engagement , fi...   \n",
       "(JUNIOR PENETRATION TESTER, C)                                                   IT   \n",
       "\n",
       "                                  ...   130   131   132   133   134   135  \\\n",
       "(JUNIOR PENETRATION TESTER, All)  ...  None  None  None  None  None  None   \n",
       "(JUNIOR PENETRATION TESTER, K)    ...  None  None  None  None  None  None   \n",
       "(JUNIOR PENETRATION TESTER, S)    ...  None  None  None  None  None  None   \n",
       "(JUNIOR PENETRATION TESTER, A)    ...  None  None  None  None  None  None   \n",
       "(JUNIOR PENETRATION TESTER, C)    ...  None  None  None  None  None  None   \n",
       "\n",
       "                                   136   137   138   139  \n",
       "(JUNIOR PENETRATION TESTER, All)  None  None  None  None  \n",
       "(JUNIOR PENETRATION TESTER, K)    None  None  None  None  \n",
       "(JUNIOR PENETRATION TESTER, S)    None  None  None  None  \n",
       "(JUNIOR PENETRATION TESTER, A)    None  None  None  None  \n",
       "(JUNIOR PENETRATION TESTER, C)    None  None  None  None  \n",
       "\n",
       "[5 rows x 140 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Job Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IT Cybersecurity Specialist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEPARTMENT OF HOMELAND SECURITY</td>\n",
       "      <td>Duties\\nSummary\\nWho May Be Considered:\\n\\nU.S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IT Cybersecurity Specialist INFOSEC</td>\n",
       "      <td>$92,143 to $141,548 per year</td>\n",
       "      <td>DEPARTMENT OF HOMELAND SECURITY</td>\n",
       "      <td>Duties\\nSummary\\nThis announcement is issued u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Title                        Salary  \\\n",
       "0          IT Cybersecurity Specialist                           NaN   \n",
       "1  IT Cybersecurity Specialist INFOSEC  $92,143 to $141,548 per year   \n",
       "\n",
       "                      Organization  \\\n",
       "0  DEPARTMENT OF HOMELAND SECURITY   \n",
       "1  DEPARTMENT OF HOMELAND SECURITY   \n",
       "\n",
       "                                     Job Description  \n",
       "0  Duties\\nSummary\\nWho May Be Considered:\\n\\nU.S...  \n",
       "1  Duties\\nSummary\\nThis announcement is issued u...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "# USAJobs\n",
    "\n",
    "# Working with 'Master_Indeed_Dataset'\n",
    "path = r'C:\\Users\\15713\\Desktop\\Datasets\\csv_xlsx\\USA Jobs Dataset.xlsx'\n",
    "df = pd.read_excel(path)\n",
    "\n",
    "df.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IT Cybersecurity Specialist</td>\n",
       "      <td>Duties\\nSummary\\nWho May Be Considered:\\n\\nU.S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IT Cybersecurity Specialist INFOSEC</td>\n",
       "      <td>Duties\\nSummary\\nThis announcement is issued u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT Cybersecurity Specialist (ENTARCH)</td>\n",
       "      <td>Duties\\nSummary\\nThis announcement is issued u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Title  \\\n",
       "0            IT Cybersecurity Specialist   \n",
       "1    IT Cybersecurity Specialist INFOSEC   \n",
       "2  IT Cybersecurity Specialist (ENTARCH)   \n",
       "\n",
       "                                         Description  \n",
       "0  Duties\\nSummary\\nWho May Be Considered:\\n\\nU.S...  \n",
       "1  Duties\\nSummary\\nThis announcement is issued u...  \n",
       "2  Duties\\nSummary\\nThis announcement is issued u...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe with title as index and description as an attribute\n",
    "title, description = df['Title'].to_list(), df['Job Description'].to_list()\n",
    "usaJ_df = pd.DataFrame(data={\"Title\": title,\n",
    "                               \"Description\": description} \n",
    "                         )\n",
    "usaJ_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the above dataframe into dictionary for iteration {title: description}\n",
    "usaJ = {}\n",
    "cnt=0\n",
    "for (t, d) in zip(title, description): # loops through the list title and description\n",
    "    if t in usaJ and t!='None': # if the job title already exists, we lump the descriptions together\n",
    "        usaJ[t]+=d\n",
    "    elif t=='None':\n",
    "        cnt+=1\n",
    "        usaJ[f'{t}_{cnt}'] = d # sets the title as key and the description as the value\n",
    "    else:\n",
    "        usaJ[t] = d\n",
    "\n",
    "\n",
    "# iterating through the Job's to tokenize into sentences/phrases\n",
    "usaJ_tokenized_dict = {}\n",
    "for key in usaJ:\n",
    "    # if key != 'None':\n",
    "    text = usaJ[key]\n",
    "    # Tokenize into sentence or phrase\n",
    "    sentence_count = get_sentence_counter(text=text)\n",
    "    usaJ_tokenized_dict[key] = sentence_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(usaJ_tokenized_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will sort out the sentences into Ability, skill, knowledge or Certs and save it as a nested dict\n",
    "USAJ_tokenized_dict = {}\n",
    "pattern = '[\\s|^|^\\s][A-Z(][A-Z]+[\\s|)|,]'\n",
    "U_dict = {}\n",
    "for k in usaJ_tokenized_dict.keys():\n",
    "    K,S,A,C,All = [],[],[],[],[]\n",
    "    for sentence in usaJ_tokenized_dict[k]:\n",
    "        # if check_word(storage,sentence):\n",
    "        s = remove_stopwords(sentence, avoid) # Removing stopwords and reconstructing sentence\n",
    "        All.append(s)\n",
    "        if check_word(bag_K,sentence):\n",
    "            s = remove_stopwords(sentence, avoid) # Removing stopwords and reconstructing sentence\n",
    "            K.append(s)\n",
    "        if check_word(bag_S,sentence):\n",
    "            s = remove_stopwords(sentence, avoid) # Removing stopwords and reconstructing sentence\n",
    "            S.append(s)\n",
    "        if check_word(bag_A,sentence):\n",
    "            s = remove_stopwords(sentence, avoid) # Removing stopwords and reconstructing sentence\n",
    "            A.append(s)\n",
    "        \n",
    "        # Here, I tried to use regex to find patterns matching a valid Certificate by first\n",
    "        # finding a sentence that contains the flag words\n",
    "        if check_word(bag_C, sentence):\n",
    "            # print(f'\\nsentence: {sentence}') #printing the sentence to be checked for valid certs\n",
    "            match = re.findall(pattern, sentence) #checking words matching our pattern\n",
    "            # print('found certs...')\n",
    "            C.append(find_cert(sentence,cert_list))\n",
    "            # print(f'-{find_cert(sentence,cert_list)}')\n",
    "            for x in match:\n",
    "                x = re.sub('\\W', '', x) #removes any non-word characters (space,symbols)\n",
    "                C.append(x) #adding cleaned up string to the certs list\n",
    "                # print(f'-{x}')\n",
    "\n",
    "  \n",
    "    USAJ_tokenized_dict[k] = {'All':All,\n",
    "                                'K':K,\n",
    "                                'S':S,\n",
    "                                'A':A,\n",
    "                                'C':list(set(C))\n",
    "                                }\n",
    "    #for visualization purose only:\n",
    "#     U_dict[k] = K+S+A+C\n",
    "with open('USAJ_tokenized_dict.json', 'w') as f:\n",
    "    json.dump(USAJ_tokenized_dict, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(USAJ_tokenized_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "# SYLLABI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with the syllabi data from NICCS\n",
    "path = r'C:\\Users\\15713\\Desktop\\Datasets\\csv_xlsx\\syllabi_niccs(final).csv'\n",
    "NICCS = pd.read_csv(path, encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NICCS.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Columns needed to list for easier itteration\n",
    "CourseTitle = NICCS['CourseTitle'].to_list()\n",
    "WorkRole = NICCS['WorkRole'].to_list()\n",
    "WorkRoleID = NICCS['WorkRoleID'].to_list()\n",
    "Tasks = NICCS['WorkRoleDesc'].to_list()\n",
    "Knowledge = NICCS['Knowledge'].to_list()\n",
    "Skill = NICCS['Skills'].to_list()\n",
    "Ability = NICCS['Abilities'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving each KSA under each work role as a nested dictionary\n",
    "Syllabi_dict = {}\n",
    "niccs_dict = {}\n",
    "pattern = '[S|A|K][0-9]+[\\S][\\s]'\n",
    "for i,WR in enumerate(WorkRole):\n",
    "    K,S,A,All =[], [], [],[]\n",
    "    if WR not in Syllabi_dict.keys(): # Avoiding duplicates\n",
    "        # splitting to get each KSA & IDs, while also removing the A####,K####, S#### tags\n",
    "        step_1 = re.sub(pattern, '', Knowledge[i]).split(\".\")\n",
    "        for sent in step_1: # first split by period\n",
    "            K+= remove_stopwords(sent, avoid).split(\";\") # take the list and split each text with ';' and store in a list\n",
    "            \n",
    "        step_1 = re.sub(pattern, '', Skill[i]).split(\".\")\n",
    "        for sent in step_1: # first split by period\n",
    "            S+= remove_stopwords(sent, avoid).split(\";\") # take the list and split each text with ';' and store in a list\n",
    "\n",
    "        step_1 = re.sub(pattern, '', Ability[i]).split(\".\")\n",
    "        for sent in step_1: # first split by period\n",
    "            A+= remove_stopwords(sent, avoid).split(\";\") # take the list and split each text with ';' and store in a list\n",
    "       \n",
    "        \n",
    "        All = K+S+A\n",
    "        \n",
    "        Syllabi_dict[WR] = {'ID': WorkRoleID[i],\n",
    "                            'All': All, \n",
    "                             'K': K, \n",
    "                             'S': S, \n",
    "                             'A': A} #Saving as a nested dict\n",
    "        # for visualization purposes only:\n",
    "        niccs_dict[WR] = [WorkRoleID[i]] + K + S + A\n",
    "        \n",
    "# Exporting dictionary as a json file for use during sentence vectorization\n",
    "with open('syllabi_NICCS.json', 'w') as f:\n",
    "    json.dump(Syllabi_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for wr in Syllabi_dict.keys():\n",
    "#     for ksa in Syllabi_dict[wr].keys():\n",
    "#         if (len(Syllabi_dict[wr][ksa]))<=3:\n",
    "#             print(Syllabi_dict[wr][ksa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Research and Development Specialist</th>\n",
       "      <td>SP-TRD-001</td>\n",
       "      <td>computer networking concepts protocols , netwo...</td>\n",
       "      <td>risk processes ( e</td>\n",
       "      <td>g</td>\n",
       "      <td>, methods assessing mitigating risk )</td>\n",
       "      <td>laws , regulations , policies , ethics relate ...</td>\n",
       "      <td>cybersecurity privacy principles</td>\n",
       "      <td>cyber threats vulnerabilities</td>\n",
       "      <td>operational impacts cybersecurity lapses</td>\n",
       "      <td>application vulnerabilities</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cyber Legal Advisor</th>\n",
       "      <td>OV-LGA-001</td>\n",
       "      <td>computer networking concepts protocols , netwo...</td>\n",
       "      <td>risk processes ( e</td>\n",
       "      <td>g</td>\n",
       "      <td>, methods assessing mitigating risk )</td>\n",
       "      <td>laws , regulations , policies , ethics relate ...</td>\n",
       "      <td>cybersecurity privacy principles</td>\n",
       "      <td>cyber threats vulnerabilities</td>\n",
       "      <td>operational impacts cybersecurity lapses</td>\n",
       "      <td>concepts practices processing digital forensic...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All Source Analyst</th>\n",
       "      <td>AN-ASA-001</td>\n",
       "      <td>computer networking concepts protocols , netwo...</td>\n",
       "      <td>risk processes ( e</td>\n",
       "      <td>g</td>\n",
       "      <td>, methods assessing mitigating risk )</td>\n",
       "      <td>laws , regulations , policies , ethics relate ...</td>\n",
       "      <td>cybersecurity privacy principles</td>\n",
       "      <td>cyber threats vulnerabilities</td>\n",
       "      <td>operational impacts cybersecurity lapses</td>\n",
       "      <td>human computer interaction principles</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            0    \\\n",
       "Research and Development Specialist  SP-TRD-001   \n",
       "Cyber Legal Advisor                  OV-LGA-001   \n",
       "All Source Analyst                   AN-ASA-001   \n",
       "\n",
       "                                                                                   1    \\\n",
       "Research and Development Specialist  computer networking concepts protocols , netwo...   \n",
       "Cyber Legal Advisor                  computer networking concepts protocols , netwo...   \n",
       "All Source Analyst                   computer networking concepts protocols , netwo...   \n",
       "\n",
       "                                                     2   3    \\\n",
       "Research and Development Specialist  risk processes ( e   g    \n",
       "Cyber Legal Advisor                  risk processes ( e   g    \n",
       "All Source Analyst                   risk processes ( e   g    \n",
       "\n",
       "                                                                        4    \\\n",
       "Research and Development Specialist  , methods assessing mitigating risk )    \n",
       "Cyber Legal Advisor                  , methods assessing mitigating risk )    \n",
       "All Source Analyst                   , methods assessing mitigating risk )    \n",
       "\n",
       "                                                                                   5    \\\n",
       "Research and Development Specialist  laws , regulations , policies , ethics relate ...   \n",
       "Cyber Legal Advisor                  laws , regulations , policies , ethics relate ...   \n",
       "All Source Analyst                   laws , regulations , policies , ethics relate ...   \n",
       "\n",
       "                                                                   6    \\\n",
       "Research and Development Specialist  cybersecurity privacy principles    \n",
       "Cyber Legal Advisor                  cybersecurity privacy principles    \n",
       "All Source Analyst                   cybersecurity privacy principles    \n",
       "\n",
       "                                                                7    \\\n",
       "Research and Development Specialist  cyber threats vulnerabilities    \n",
       "Cyber Legal Advisor                  cyber threats vulnerabilities    \n",
       "All Source Analyst                   cyber threats vulnerabilities    \n",
       "\n",
       "                                                                           8    \\\n",
       "Research and Development Specialist  operational impacts cybersecurity lapses    \n",
       "Cyber Legal Advisor                  operational impacts cybersecurity lapses    \n",
       "All Source Analyst                   operational impacts cybersecurity lapses    \n",
       "\n",
       "                                                                                   9    \\\n",
       "Research and Development Specialist                       application vulnerabilities    \n",
       "Cyber Legal Advisor                  concepts practices processing digital forensic...   \n",
       "All Source Analyst                              human computer interaction principles    \n",
       "\n",
       "                                     ...  169  170  171  172  173  174  175  \\\n",
       "Research and Development Specialist  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "Cyber Legal Advisor                  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "All Source Analyst                   ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "                                     176  177  178  \n",
       "Research and Development Specialist  NaN  NaN  NaN  \n",
       "Cyber Legal Advisor                  NaN  NaN  NaN  \n",
       "All Source Analyst                   NaN  NaN  NaN  \n",
       "\n",
       "[3 rows x 179 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "niccs_concat = pd.DataFrame()\n",
    "for k,v in niccs_dict.items():\n",
    "    if k != 'None':\n",
    "        df = pd.DataFrame({k: v})\n",
    "        niccs_concat = pd.concat([niccs_concat, df], axis=1)\n",
    "\n",
    "niccs_concat.T.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllabi_niccs_df =  pd.DataFrame.from_dict({(i,j): Syllabi_dict[i][j] \n",
    "                           for i in Syllabi_dict.keys() \n",
    "                           for j in Syllabi_dict[i].keys()},\n",
    "                       orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(Network Operations Specialist, ID)</th>\n",
       "      <td>OM-NET-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(Network Operations Specialist, All)</th>\n",
       "      <td>[computer networking concepts protocols , netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(Network Operations Specialist, K)</th>\n",
       "      <td>[computer networking concepts protocols , netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(Network Operations Specialist, S)</th>\n",
       "      <td>[skill analyzing network traffic capacity perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(Network Operations Specialist, A)</th>\n",
       "      <td>[operate network equipment hubs , routers , sw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      0\n",
       "(Network Operations Specialist, ID)                                          OM-NET-001\n",
       "(Network Operations Specialist, All)  [computer networking concepts protocols , netw...\n",
       "(Network Operations Specialist, K)    [computer networking concepts protocols , netw...\n",
       "(Network Operations Specialist, S)    [skill analyzing network traffic capacity perf...\n",
       "(Network Operations Specialist, A)    [operate network equipment hubs , routers , sw..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllabi_niccs_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "################################################################################\n",
    "# Certs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with the Certificate data from NICCS\n",
    "path = r'C:\\Users\\15713\\Desktop\\Datasets\\csv_xlsx\\cert_list.xlsx' \n",
    "Certs = pd.read_excel(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cert Name</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Time (hrs)</th>\n",
       "      <th>Avg Salary of Recipient</th>\n",
       "      <th>Number of Holders</th>\n",
       "      <th>Knowledge</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Abilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CISSP (Certified Information Systems Security ...</td>\n",
       "      <td>699</td>\n",
       "      <td>40-50</td>\n",
       "      <td>122138.0</td>\n",
       "      <td>92976.0</td>\n",
       "      <td>K0001: Knowledge of computer networking concep...</td>\n",
       "      <td>S0011: Skill in conducting information searche...</td>\n",
       "      <td>A0002: Ability to match the appropriate knowle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CISA (Certified Information Systems Auditor)</td>\n",
       "      <td>595</td>\n",
       "      <td>112</td>\n",
       "      <td>132278.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>K0001: Knowledge of computer networking concep...</td>\n",
       "      <td>S0027: Skill in determining how a security sys...</td>\n",
       "      <td>A0162: Ability to recognize the unique aspects...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Cert Name  Cost Time (hrs)  \\\n",
       "0  CISSP (Certified Information Systems Security ...   699      40-50   \n",
       "1       CISA (Certified Information Systems Auditor)   595        112   \n",
       "\n",
       "   Avg Salary of Recipient  Number of Holders  \\\n",
       "0                 122138.0            92976.0   \n",
       "1                 132278.0           150000.0   \n",
       "\n",
       "                                           Knowledge  \\\n",
       "0  K0001: Knowledge of computer networking concep...   \n",
       "1  K0001: Knowledge of computer networking concep...   \n",
       "\n",
       "                                              Skills  \\\n",
       "0  S0011: Skill in conducting information searche...   \n",
       "1  S0027: Skill in determining how a security sys...   \n",
       "\n",
       "                                          Abilities   \n",
       "0  A0002: Ability to match the appropriate knowle...  \n",
       "1  A0162: Ability to recognize the unique aspects...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Certs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cert_name = Certs['Cert Name'].to_list()\n",
    "c_K = Certs['Knowledge'].to_list()\n",
    "c_S = Certs['Skills'].to_list()\n",
    "c_A = Certs['Abilities '].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving each KSA under each Cert as a nested dictionary\n",
    "Cert_dict = {}\n",
    "c_dict = {}\n",
    "pattern = '[S|A|K][0-9]+[\\S][\\s]' \n",
    "for i,cert in enumerate(cert_name):\n",
    "    K,S,A,All= [],[],[], []\n",
    "    if cert not in Cert_dict.keys(): # Avoiding duplicates\n",
    "        k = 'None' if pd.isna(c_K[i]) else re.sub(pattern, '', c_K[i]).split(\"\\n\")\n",
    "        s = 'None' if pd.isna(c_S[i]) else re.sub(pattern, '', c_S[i]).split(\"\\n\")\n",
    "        a = 'None' if pd.isna(c_A[i]) else re.sub(pattern, '', c_A[i]).split(\"\\n\")\n",
    "        \n",
    "        for sentence in k:\n",
    "            # Removing stopwords and reconstructing sentence\n",
    "            K.append(remove_stopwords(sentence, avoid))\n",
    "        for sentence in s:\n",
    "            # Removing stopwords and reconstructing sentence\n",
    "            S.append(remove_stopwords(sentence, avoid))\n",
    "        for sentence in a:\n",
    "            # Removing stopwords and reconstructing sentence\n",
    "            A.append(remove_stopwords(sentence, avoid))\n",
    "        All = K+S+A\n",
    "        Cert_dict[cert] = {'All':All,\n",
    "                           'K': K, \n",
    "                           'S': S, \n",
    "                           'A': A} \n",
    "        #Saving as a nested dict\n",
    "        #for visualization purpose only:\n",
    "        c_dict[cert] = All\n",
    "# Exporting data\n",
    "with open('Certs.json', 'w') as f:\n",
    "    json.dump(Cert_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_dict = pd.DataFrame()\n",
    "# for k,v in Cert_dict.items():\n",
    "#     if k != 'None':\n",
    "#         df = pd.DataFrame({k: v})\n",
    "#         c_dict = pd.concat([c_dict, df], axis=1)\n",
    "\n",
    "# c_dict.T.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cert_niccs_df =  pd.DataFrame.from_dict({(i,j): Cert_dict[i][j] \n",
    "                           for i in Cert_dict.keys() \n",
    "                           for j in Cert_dict[i].keys()},\n",
    "                       orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cert_dict['CISA (Certified Information Systems Auditor)']['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## HERE I JUST WANTED TO TRY TO GROUP SIMILAR JOB-TITLES(JOB POSTINGS) AND JOB-ROLES(SYLLABI) TOGETHER\n",
    "############## USING \"THE MOST COMMON WORD\" TECHNIQUE\n",
    "# function that returns the number of common words in two sentences\n",
    "\n",
    "def common_words_count(s1, s2):\n",
    "    count = 0\n",
    "    # Convert sentence to all lower case and split them into words\n",
    "    try:\n",
    "        l1 = s1.lower().split(' ') if s1==s1 else None\n",
    "        l2 = s2.lower().split(' ') if s2==s2 else None\n",
    "\n",
    "        #Lemmetization if necessary (but for now, i will try it without)\n",
    "\n",
    "        if l1 is not None:\n",
    "            for w1 in l1:\n",
    "                if l2 is not None and w1 in l2:\n",
    "                    count+=1\n",
    "    except:\n",
    "        pass\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I tried grouping similar jobs together just by looking at their titles\n",
    "# using the common words they share. But as there are plenty of jobs that share\n",
    "# similar names but have completely different KSAs, I didn't want to risk lumping different\n",
    "# work roles together just because they shared a title. Therefore, We didn't follow up with this method\n",
    "min_count = 4\n",
    "\n",
    "similar = {}\n",
    "others = []\n",
    "for K in Master_indeed_dict.keys():\n",
    "#     print(K)\n",
    "#     print(sim)\n",
    "    for k in WR:\n",
    "        sim = []\n",
    "        if k==k and K==K: # making sure that we don't have a nan \n",
    "            K_words = K.lower().split(' ')\n",
    "            k_words = k.lower().split(' ')\n",
    "            \n",
    "                                 \n",
    "            if len(K_words) > len(k_words):\n",
    "                # assumption: We can not have more common words than the word count of the smallest job-title\n",
    "                min_count = len(k_words) if min_count > len(k_words) else min_count\n",
    "                for word in K_words:\n",
    "                    if word in k_words:\n",
    "                        sim.append(word)\n",
    "            else:\n",
    "                # assumption: We can not have more common words than the word count of the smallest job-title\n",
    "                min_count = len(K_words) if min_count > len(K_words) else min_count\n",
    "                for word in k_words:\n",
    "                    if word in K_words:\n",
    "                        sim.append(word)\n",
    "            if len(sim) >= min_count:\n",
    "                if k not in similar.keys():\n",
    "                    similar[k] = []\n",
    "                    similar[k].append(K)\n",
    "                else:\n",
    "                    similar[k].append(K)\n",
    "            else: #if the job title doesn't fall under the 52 work roles\n",
    "                others.append(K)\n",
    "            \n",
    "# Exporting dictionary as a json file for use during sentence vectorization\n",
    "data = similar\n",
    "with open('similar.json', 'w') as f:\n",
    "    json.dump(data, f)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# others\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
